{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e346757-646f-49c9-a2d4-d66ced00f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from my_ANN import initialize_params, forward, compute_cost, backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bff4b02-b554-4229-92be-01b15ebfe885",
   "metadata": {},
   "source": [
    "### Convert all Parameters into a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb1d513f-8bb0-46b6-9b15-4e5a37b8b40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.83518171, -0.60852509],\n",
       "        [ 1.11130172, -0.50993656],\n",
       "        [-0.34327987,  0.39759152]]),\n",
       " 'B1': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.16970138, -0.39428035, -0.88987278]]),\n",
       " 'B2': array([[0.]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = initialize_params([2,3,1])\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe8fc33-2d69-45ea-8b7f-e5943a1c6096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W1', 'B1', 'W2', 'B2']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_keys = sorted(params.keys(), key=lambda x: (-int(x[1]), x[0]), reverse=True)  # descending\n",
    "sorted_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6681b88a-5a93-4166-b8a1-937109ff5151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.83518171, -0.60852509],\n",
       "        [ 1.11130172, -0.50993656],\n",
       "        [-0.34327987,  0.39759152]]),\n",
       " 'W2': array([[ 0.16970138, -0.39428035, -0.88987278]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_W = {key: value for key, value in params.items() if not key.startswith('B')}\n",
    "params_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c723877d-d874-40bc-9dfa-cb5be6fe3503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_to_vector(params):\n",
    "    param_vector = []\n",
    "    param_shape_dict = {}\n",
    "    \n",
    "    sorted_keys = sorted(params.keys(), key=lambda x: (-int(x[1]), x[0]), reverse=True)  # descending\n",
    "    for key in sorted_keys:\n",
    "        param_vector.append(params[key].flatten())\n",
    "        param_shape_dict[key] = params[key].shape\n",
    "        \n",
    "    param_vector = np.concatenate(param_vector, axis=0)\n",
    "    \n",
    "    return param_vector, param_shape_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97f5a74a-3349-4338-bcbf-bccefdad221f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.83518171, -0.60852509,  1.11130172, -0.50993656, -0.34327987,\n",
       "         0.39759152,  0.        ,  0.        ,  0.        ,  0.16970138,\n",
       "        -0.39428035, -0.88987278,  0.        ]),\n",
       " {'W1': (3, 2), 'B1': (3, 1), 'W2': (1, 3), 'B2': (1, 1)})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_vector, param_shape_dict = params_to_vector(params)\n",
    "param_vector, param_shape_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33dad45-9318-408a-b858-82048dfc2514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads_to_vector(grads):\n",
    "    grad_vector = []\n",
    "    \n",
    "    # Remove keys that start with 'dA'\n",
    "    grads = {key: value \n",
    "             for key, value in grads.items() \n",
    "             if not key.startswith('dA')}\n",
    "    \n",
    "    sorted_keys = sorted(grads.keys(), key=lambda x: (-int(x[2]), x[1]), reverse=True)  # dW1, dB1, dW2, dB2, ...\n",
    "    for key in sorted_keys:\n",
    "        grad_vector.append(grads[key].flatten())\n",
    "    \n",
    "    grad_vector = np.concatenate(grad_vector, axis=0)\n",
    "    \n",
    "    return grad_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c253b296-898f-4298-b220-4e6712015e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.94752275,  1.59906321,  0.53015376,  1.12324794, -0.61308952,\n",
       "        0.42788732,  0.05212445,  0.84834142, -0.63920017, -0.73756782,\n",
       "       -1.19341761, -1.58130931, -0.046875  ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = {\n",
    "    'dW2': np.random.randn(3, 2),\n",
    "    'dB2': np.random.randn(3, 1),\n",
    "    'dW1': np.random.randn(1, 3),\n",
    "    'dB1': np.random.randn(1, 1),\n",
    "}\n",
    "\n",
    "grads_to_vector(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0c450-3a57-4ea1-a7b1-756a3dc43fc0",
   "metadata": {},
   "source": [
    "### Inverse Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f76d5bcf-a7e5-46db-b098-70fbcaa2f245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(param_shape_dict['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08a43fcf-611b-40c7-8659-11f52811152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_dict(vector, shape_dict):\n",
    "    dict = {}\n",
    "\n",
    "    start_idx = 0\n",
    "    for key, shape in shape_dict.items():\n",
    "        size = np.prod(shape)\n",
    "        end_idx = start_idx + size\n",
    "        \n",
    "        dict[key] = vector[start_idx: end_idx].reshape(shape)\n",
    "\n",
    "        start_idx += size\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee224279-348d-4588-9e1f-7b0cf4aae888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.83518171, -0.60852509],\n",
       "        [ 1.11130172, -0.50993656],\n",
       "        [-0.34327987,  0.39759152]]),\n",
       " 'B1': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.16970138, -0.39428035, -0.88987278]]),\n",
       " 'B2': array([[0.]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_to_dict(param_vector, param_shape_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c12b96-3134-4e84-a4c2-a8e4a154be31",
   "metadata": {},
   "source": [
    "# Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b45c66-22e8-41a9-97b9-2448b86574af",
   "metadata": {},
   "source": [
    "$$ gradapprox = \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$$\n",
    "\n",
    "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 }$$\n",
    "\n",
    "**Note**: Use `np.linalg.norm` to get the norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d725393-9dce-47b6-94cd-93245882fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradient(X, Y, params, grads, epsilon=1e-7, print_process=False):\n",
    "    param_vector, param_shape_dict = params_to_vector(params)  # shape (-1, )\n",
    "    grad_vector = grads_to_vector(grads)  # shape (-1, )\n",
    "\n",
    "    n_params = param_vector.shape[0]\n",
    "    J_plus = np.zeros((n_params, ))\n",
    "    J_minus = np.zeros((n_params, ))\n",
    "    grad_approx_vector = np.zeros((n_params, ))\n",
    "\n",
    "    for i in range(n_params):  # we want to change only 1 param and keep others intact to compute the change of cost based on that 1 param\n",
    "        # Perturb the i-th parameter positively\n",
    "        plus_param_vector = np.copy(param_vector)\n",
    "        plus_param_vector[i] +=  epsilon\n",
    "        \n",
    "        Y_pred1, _ = forward(X, vector_to_dict(plus_param_vector, param_shape_dict))\n",
    "        J_plus[i] = compute_cost(Y_pred1, Y)\n",
    "        \n",
    "        # Perturb the i-th parameter negatively\n",
    "        minus_param_vector = np.copy(param_vector)\n",
    "        minus_param_vector[i] -= epsilon\n",
    "        \n",
    "        Y_pred2, _ = forward(X, vector_to_dict(minus_param_vector, param_shape_dict))\n",
    "        J_minus[i] = compute_cost(Y_pred2, Y)\n",
    "        \n",
    "        # Compute the approximate gradient\n",
    "        grad_approx_vector[i] = (J_plus[i] - J_minus[i]) / (2*epsilon)\n",
    "\n",
    "        if print_process==True and i % 10 == 0:\n",
    "            print(f\"Parameter {i}:\")\n",
    "            print(f\"plus_param_vector: {plus_param_vector[i]}\")\n",
    "            print(f\"J_plus: {J_plus[i]}\")\n",
    "            print(f\"minus_param_vector: {minus_param_vector[i]}\")\n",
    "            print(f\"J_minus: {J_minus[i]}\")\n",
    "            print(f\"grad_approx_vector: {grad_approx_vector[i]}\")\n",
    "            print('-'*100)\n",
    "\n",
    "    # Compare gradients\n",
    "    numerator = np.linalg.norm(grad_vector - grad_approx_vector)\n",
    "    denominator = np.linalg.norm(grad_approx_vector) + np.linalg.norm(grad_vector)\n",
    "    diff = numerator / denominator\n",
    "\n",
    "    if diff > 2e-7:\n",
    "        print (\"There is a mistake in the backward propagation! Difference = \" + str(diff))\n",
    "    else:\n",
    "        print (\"Your backward propagation works perfectly fine! Difference = \" + str(diff))\n",
    "\n",
    "    return diff, grad_approx_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "233273f2-e17f-40c8-8b80-fe11da1685ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_case(): \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    x = np.random.randn(4,3)\n",
    "    y = np.array([1, 1, 0]).reshape(1, -1)\n",
    "    \n",
    "    W1 = np.random.randn(5,4) \n",
    "    B1 = np.random.randn(5,1) \n",
    "    W2 = np.random.randn(3,5) \n",
    "    B2 = np.random.randn(3,1) \n",
    "    W3 = np.random.randn(1,3) \n",
    "    B3 = np.random.randn(1,1) \n",
    "    parameters = {\n",
    "        \"W1\": W1, \"B1\": B1,\n",
    "        \"W2\": W2, \"B2\": B2,\n",
    "        \"W3\": W3, \"B3\": B3\n",
    "    }\n",
    "    \n",
    "    return x, y, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90039792-e4ad-42d9-8353-8d8d1e14c6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.37347779, -1.47903216,  0.17596143, -1.33685036, -0.01967514,\n",
       "       -0.08573553,  0.01188465, -0.07674312,  0.03916037, -0.05539735,\n",
       "        0.04872715, -0.09359393, -0.05337778, -0.21138458,  0.02514856,\n",
       "       -0.19106384,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.63290787,  0.0372514 , -0.06401301,  0.09045575,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.91580165,  0.02451548, -0.10797954,  0.90281891,  0.        ,\n",
       "        0.        ,  0.        ,  0.19763343,  0.        ,  0.        ,\n",
       "        2.24404238,  0.21225753])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y, parameters = test_case()\n",
    "\n",
    "# forward pass\n",
    "Y_pred, caches = forward(X, parameters)\n",
    "# back-propagation\n",
    "gradients = backward(Y, Y_pred, caches)\n",
    "\n",
    "grads_to_vector(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c25e12b-48fe-4c4f-82cf-d1e5eb709cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your backward propagation works perfectly fine! Difference = 1.1890417878730741e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.1890417878730741e-07,\n",
       " array([-0.37347774, -1.47903191,  0.17596133, -1.33685053, -0.01967497,\n",
       "        -0.08573522,  0.01188466, -0.07674277,  0.03916037, -0.05539735,\n",
       "         0.04872715, -0.09359393, -0.05337767, -0.21138479,  0.02514863,\n",
       "        -0.19106371,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.63290795,  0.03725111, -0.06401301,  0.09045546,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.9158017 ,  0.02451541, -0.10797954,  0.90281879,  0.        ,\n",
       "         0.        ,  0.        ,  0.19763344,  0.        ,  0.        ,\n",
       "         2.24404227,  0.21225742]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_gradient(X, Y, parameters, gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14b148b9-8e23-45eb-8281-6be55052be37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter 0:\n",
      "plus_param_vector: -0.3224171040135075\n",
      "J_plus: 2.4078333524866014\n",
      "minus_param_vector: -0.3224173040135075\n",
      "J_minus: 2.4078334271821493\n",
      "grad_approx_vector: -0.37347773984564014\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parameter 10:\n",
      "plus_param_vector: 0.9015908205927955\n",
      "J_plus: 2.4078333947070716\n",
      "minus_param_vector: 0.9015906205927956\n",
      "J_minus: 2.4078333849616422\n",
      "grad_approx_vector: 0.048727146761962103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parameter 20:\n",
      "plus_param_vector: -0.6871726001195995\n",
      "J_plus: 2.4078334531251744\n",
      "minus_param_vector: -0.6871728001195994\n",
      "J_minus: 2.407833326543585\n",
      "grad_approx_vector: 0.6329079460520859\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parameter 30:\n",
      "plus_param_vector: -0.7471581937508377\n",
      "J_plus: 2.407833389834357\n",
      "minus_param_vector: -0.7471583937508376\n",
      "J_minus: 2.407833389834357\n",
      "grad_approx_vector: 0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parameter 40:\n",
      "plus_param_vector: -1.14251809802214\n",
      "J_plus: 2.407833389834357\n",
      "minus_param_vector: -1.1425182980221402\n",
      "J_minus: 2.407833389834357\n",
      "grad_approx_vector: 0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Your backward propagation works perfectly fine! Difference = 1.1890417878730741e-07\n"
     ]
    }
   ],
   "source": [
    "_, _ = check_gradient(X, Y, parameters, gradients, print_process=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6e7ad7-d7b8-4a5a-9e31-00f150fe669b",
   "metadata": {},
   "source": [
    "**Notes** \n",
    "- Gradient Checking is slow! Approximating the gradient with $\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ is computationally costly. For this reason, we don't run gradient checking at every iteration during training. Just a few times to check if the gradient is correct. \n",
    "- Gradient Checking, at least as we've presented it, doesn't work with dropout. You would usually run the gradient check algorithm without dropout to make sure your backprop is correct, then add dropout. \n",
    "\n",
    "Congrats! Now you can be confident that your deep learning model is working correctly! You can even use this to convince your CEO. :) \n",
    "<br>\n",
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember from this notebook**:\n",
    "- Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation).\n",
    "- Gradient checking is slow, so you don't want to run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf7bc0-e3ad-404c-b1de-4d273a6736a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
